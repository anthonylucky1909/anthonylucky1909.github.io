# Understanding Recurrent Neural Networks (RNN) in Deep Learning

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![Python](https://img.shields.io/badge/Python-3.8+-3776AB.svg?logo=python\&logoColor=white)](https://www.python.org/) [![PyTorch](https://img.shields.io/badge/PyTorch-2.1.0-EE4C2C.svg)](https://pytorch.org/)

---

## Overview
![Transformer Decoding](https://jalammar.github.io/images/t/transformer_decoding_2.gif)

Recurrent Neural Networks (RNNs) are a class of neural networks designed to handle sequential data. Unlike traditional feedforward networks that treat inputs independently, RNNs maintain a hidden state that acts as memory, capturing information from previous time steps. This architecture allows RNNs to model temporal dependencies, making them particularly effective for tasks such as language modeling, speech recognition, and time series forecasting. The RNN operates by taking the current input and the previous hidden state to produce a new hidden state and an output. This step-by-step processing over sequences is illustrated in the animated GIF [here](https://miro.medium.com/v2/0*oY-GwnsZDEaHdVyf.gif), which visually demonstrates how information flows through time steps and is updated at each stage.

Mathematically, an RNN updates its hidden state $h_t$ and output $y_t$ at time step $t$ as follows:

$$
h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h) \\
y_t = g(W_{hy} h_t + b_y)
$$

Here, $x_t$ is the input at time $t$, $h_{t-1}$ is the previous hidden state, $W_{xh}$, $W_{hh}$, and $W_{hy}$ are learnable weight matrices, $b_h$ and $b_y$ are biases, $f$ is a nonlinear activation function such as $tanh$ or $ReLU$, and $g$ is typically the softmax or identity function depending on the task.

Traditional RNNs face challenges with long-term dependencies due to vanishing or exploding gradients. To mitigate this, LSTM (Long Short-Term Memory) networks and GRUs (Gated Recurrent Units) introduce gating mechanisms to control the flow of information and retain relevant signals over longer sequences.

---

## RNN Architecture

An RNN processes sequences one step at a time, updating its hidden state at each time step based on the current input and previous hidden state. The output can be taken at each step or only at the final step, depending on the application. LSTMs introduce three gates: the input gate, forget gate, and output gate, which regulate the addition, removal, and output of information in the cell state. GRUs simplify this mechanism by combining gates into update and reset gates. These gates enable the network to maintain long-term dependencies while avoiding gradient vanishing or explosion.

The operations of a basic RNN, LSTM, and GRU can be visualized using the GIF linked above, which shows the flow of information and memory updates over time steps.

---

## RNN Implementation in PyTorch

The following PyTorch implementation defines a simple RNN model that can be used for sequence modeling tasks. It contains an RNN layer followed by a fully connected layer for mapping the last hidden state to the output.

```python
import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(SimpleRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.rnn(x)
        out = self.fc(out[:, -1, :])  # Using last time step's output
        return out

# Example usage:
# rnn = SimpleRNN(input_size=10, hidden_size=50, output_size=1)
# output = rnn(torch.randn(32, 20, 10))  # batch_size=32, seq_len=20, input_size=10
```

This implementation demonstrates how sequential inputs are processed, how the hidden state evolves over time, and how the final output can be generated for prediction tasks. For more complex sequence dependencies, LSTM or GRU layers can replace the basic RNN layer.

---

## Applications and Advantages

RNNs are widely used for tasks that involve sequential data, including language modeling, speech recognition, time series forecasting, and machine translation. They capture temporal dependencies effectively and provide an intuitive mechanism for sequence modeling. While traditional RNNs struggle with long sequences, LSTM and GRU architectures overcome these limitations, making them robust for practical applications. Understanding RNNs and their mathematical foundations provides essential insight into sequence modeling and prepares for studying more advanced architectures.

---

## References

Goodfellow, I., Bengio, Y., and Courville, A. (2016). *Deep Learning*. MIT Press.
Hochreiter, S., and Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural Computation, 9(8), 1735-1780.
PyTorch Documentation. [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)
Animated RNN GIF. [https://miro.medium.com/v2/0\*oY-GwnsZDEaHdVyf.gif](https://miro.medium.com/v2/0*oY-GwnsZDEaHdVyf.gif)
