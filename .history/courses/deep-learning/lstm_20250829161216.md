# Long Short-Term Memory (LSTM) Networks

Long Short-Term Memory (LSTM) networks are a type of recurrent neural network that effectively capture long-term dependencies in sequential data, solving the vanishing gradient problem in standard RNNs. LSTMs use gates to control the flow of information, allowing selective retention or forgetting of past states.

For input $x_t$ at time step $t$, previous hidden state $h_{t-1}$, and previous cell state $c_{t-1}$, the LSTM calculates its gates and states as follows:

The forget gate $f_t$ determines what to discard from the previous cell state:

$$
f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)
$$

The input gate $i_t$ controls what new information to add, and the candidate cell state $\tilde{c}_t$ contains potential new content:

$$
i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)
$$

Here, the \(\tanh\) activation ensures that the candidate values are constrained between \(-1\) and \(1\), providing stability when updating the cell state.


The cell state is updated by combining the previous state and new candidate information:

$$
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
$$

The output gate $o_t$ determines which parts of the cell state are exposed to the hidden state $h_t$:

$$
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)
$$

$$
h_t = o_t \odot \tanh(c_t)
$$

Here, $\sigma$ is the sigmoid function, $\tanh$ is the hyperbolic tangent, $[h_{t-1}, x_t]$ denotes concatenation, and $\odot$ represents element-wise multiplication. This precise notation makes the gating operations clear and intuitive.

LSTMs are particularly effective for language modeling, time series prediction, speech recognition, and machine translation, as they maintain relevant information over long sequences.

Below is a PyTorch implementation of an LSTM model:

```python
import torch
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.1):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Example usage
batch_size = 16
seq_len = 10
input_size = 8
hidden_size = 32
num_layers = 2
output_size = 1

x = torch.randn(batch_size, seq_len, input_size)
model = LSTMModel(input_size, hidden_size, num_layers, output_size)
output = model(x)
print(output.shape)  # [batch_size, output_size]
```

The LSTM architecture enables robust modeling of sequential data, outperforming traditional RNNs for tasks that require remembering information over extended time steps.

![LSTM GIF](https://miro.medium.com/v2/0*oY-GwnsZDEaHdVyf.gif)
