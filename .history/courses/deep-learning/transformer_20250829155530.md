# Advanced Understanding of Transformer Models in Deep Learning

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![Python](https://img.shields.io/badge/Python-3.8+-3776AB.svg?logo=python\&logoColor=white)](https://www.python.org/) [![PyTorch](https://img.shields.io/badge/PyTorch-2.1.0-EE4C2C.svg)](https://pytorch.org/)

---

## Overview

Transformers are a foundational architecture in modern deep learning, especially in natural language processing (NLP) and sequence modeling. Introduced in **“Attention Is All You Need”** (Vaswani et al., 2017), Transformers use **self-attention** mechanisms to model relationships between all elements of a sequence simultaneously. This allows them to overcome the sequential limitations of RNNs and LSTMs, making training more parallelizable and efficient.

Unlike decision trees, which rely on recursive partitioning of features, Transformers learn **representations for sequences** through layers of attention and feed-forward networks. This approach has enabled breakthroughs in machine translation, text summarization, and large language models (LLMs) such as GPT and BERT.

---

## Transformer Architecture

Transformers consist of two main parts:

1. **Encoder**: Processes the input sequence and generates contextual embeddings.
2. **Decoder**: Generates the output sequence using encoder embeddings and previous outputs.

Key components include:

* **Embedding Layers**: Map tokens to continuous vectors.
* **Positional Encoding**: Adds sequence order information.
* **Multi-Head Self-Attention**: Allows each token to attend to all other tokens in the sequence.
* **Feed-Forward Networks**: Transform the attended representations.
* **Layer Normalization and Residual Connections**: Stabilize training.

![Transformer Decoding](https://jalammar.github.io/images/t/transformer_decoding_2.gif)

---

## Positional Encoding

Since Transformers do not process sequences sequentially, **positional encoding** is added to embeddings:

$$
PE_{(pos,2i)} = \sin(pos / 10000^{2i/d_{model}})
$$

$$
PE_{(pos,2i+1)} = \cos(pos / 10000^{2i/d_{model}})
$$

This injects information about the relative position of tokens into the model.

---

## Self-Attention Mechanism

Self-attention computes a weighted sum of all tokens for each position:

$$
Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

* **Q**: Query vectors
* **K**: Key vectors
* **V**: Value vectors
* **d\_k**: Dimension of the key vectors

Multi-head attention allows the model to focus on different parts of the sequence simultaneously.

---

## Transformer Implementation in PyTorch

```python
import torch
import torch.nn as nn
import math

from positional import PositionalEncoding
from encoder import EncoderLayer
from decoder import DecoderLayer

class Transformer(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, num_encoder_layers,
                 num_decoder_layers, input_vocab_size, target_vocab_size,
                 max_seq_len, dropout=0.1):
        super().__init__()
        self.embed_dim = embed_dim
        self.input_embedding = nn.Embedding(input_vocab_size, embed_dim)
        self.target_embedding = nn.Embedding(target_vocab_size, embed_dim)
        self.positional_encoding = PositionalEncoding(embed_dim, max_seq_len)

        self.encoder_layers = nn.ModuleList([
            EncoderLayer(embed_dim, num_heads, ff_dim, dropout)
            for _ in range(num_encoder_layers)
        ])
        
        self.decoder_layers = nn.ModuleList([
            DecoderLayer(embed_dim, num_heads, ff_dim, dropout)
            for _ in range(num_decoder_layers)
        ])

        self.output_linear = nn.Linear(embed_dim, target_vocab_size)

    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        enc_x = self.input_embedding(src) * math.sqrt(self.embed_dim)
        enc_x = self.positional_encoding(enc_x)

        dec_x = self.target_embedding(tgt) * math.sqrt(self.embed_dim)
        dec_x = self.positional_encoding(dec_x)

        for layer in self.encoder_layers:
            enc_x = layer(enc_x, mask=None)

        for layer in self.decoder_layers:
            dec_x = layer(dec_x, enc_x, tgt_mask=tgt_mask, src_mask=src_mask)

        output = self.output_linear(dec_x)
        return output
```

---

## Advantages of Transformers

* Highly parallelizable training compared to RNNs.
* Captures long-range dependencies in sequences.
* Scales efficiently to large datasets.
* Basis of modern NLP and multimodal models.

---

## References

* Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). *Attention Is All You Need*. [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
* Jay Alammar. *The Illustrated Transformer*. [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)
* PyTorch Documentation. [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)

---
